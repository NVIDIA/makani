# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Optional, List
import progressbar
import os
import re
import json
import time
import pickle
import numpy as np
import h5py as h5
import datetime as dt
import argparse as ap
from glob import glob
import xarray as xr
import dask.array as da

# MPI
from mpi4py import MPI

from makani.utils.features import get_channel_groups
from makani.utils.dataloaders.data_helpers import get_date_from_timestamp

from wb2_helpers import surface_variables, atmospheric_variables, split_convert_channel_names


def convert(file_names_to_convert: List[str], output_file: str, batch_size: Optional[int]=32,
            entry_key: Optional[str]='fields', verbose: Optional[bool]=False):

    """Function to convert rollouts generated by the makani inference module to Weatherbench 2 format.  

    This function reads all files from the input_path and generates a WB2 compatible output file which
    is stored as specified in output_file.

    This routine supports distributed processing via mpi4py.
    ...

    Parameters
    ----------
    file_names_to_convert : List[str]
        Path which contains all makani compatible HDF5 files to be converted to WB2 format. The dataset inputs should be 6 dimensional,
        with dimenstions (ic, lead_time, ensemble, channel, latitude, longitude)
    output_file : str
        Output file where WB2 compatible data will be stored.
    batch_size : int
        Batch size in which the samples are processed. This does not have any effect on the statistics (besides small numerical changes because of order of operations), but
        is merely a performance setting. Bigger batches are more efficient but require more memory.
    entry_key: str
        This is the HDF5 dataset name of the data in the files. Defaults to "fields".  
    verbose : bool
        Enable for more printing.
    """


    # get comm ranks and size
    comm = MPI.COMM_WORLD.Dup()
    comm_rank = comm.Get_rank()
    comm_size = comm.Get_size()
    
    # timer
    start_time = time.perf_counter()

    # scan all files:
    dataset_shape = None
    dataset_dtype = None
    lead_times = None
    channel_names = None
    latitudes = None
    longitudes = None
    entries_per_year = []
    timestamps = []
    if comm_rank == 0:
        for idx, fname in enumerate(file_names_to_convert):
            with h5.File(fname, 'r') as f:
                if idx == 0:
                    # shapes and type
                    dataset_shape = f[entry_key].shape
                    dataset_dtype = f[entry_key].dtype

                    # get scales
                    lead_times = f[entry_key].dims[1]["lead_time"][...]
                    channel_names = f[entry_key].dims[3]["channel"][...]
                    latitudes = f[entry_key].dims[4]["lat"][...]
                    longitudes = f[entry_key].dims[5]["lon"][...]
                timestamps.append(f[entry_key].dims[0]["timestamp"][...])
                entries_per_year.append(f[entry_key].shape[0])

    # communicate
    dataset_shape = comm.bcast(dataset_shape, root=0)
    dataset_dtype = comm.bcast(dataset_dtype, root=0)
    lead_times = comm.bcast(lead_times, root=0)
    channel_names = comm.bcast(channel_names, root=0)
    latitudes = comm.bcast(latitudes, root=0)
    longitudes = comm.bcast(longitudes, root=0)
    timestamps = comm.bcast(timestamps, root=0)
    entries_per_year = comm.bcast(entries_per_year, root=0)

    # IMPORTANT! ECMWF convention flips the latitudes, so that they start on the south pole
    # we use co-latitude definition, where 90 degrees is the north pole
    latitudes = np.flip(latitudes)
                
    # total hours:
    total_entries = sum(entries_per_year)
    _, lead_time, ensemble_size, _, nlat, nlon = dataset_shape

    # convert timestamps and lead times
    timestamps = np.concatenate(timestamps, axis=0)
    date_fn = np.vectorize(get_date_from_timestamp)
    timestamps = np.array(date_fn(timestamps.tolist())).astype(np.datetime64)
    hconvert = np.vectorize(lambda x: np.timedelta64(x, 'h'))
    lead_times = lead_times.astype('timedelta64[h]').astype('timedelta64[ns]')

    # convert channel names: split by surface vs atmospheric:
    channel_names = [c.decode("ascii").strip() for c in channel_names.tolist()]

    # split
    atmospheric_channel_names, atmospheric_channel_names_wb2, surface_channel_names, surface_channel_names_wb2, atmospheric_levels = split_convert_channel_names(channel_names)
    nlevels = len(atmospheric_levels)

    if comm_rank == 0:
        print( f"Converting files with {(total_entries,) + dataset_shape[1:]} to WB2 format.")

        # create zarr file
        data_arrays = {}
        for sc in surface_channel_names_wb2:
            data_arrays[sc] = (["time", "number", "prediction_timedelta", "latitude", "longitude"],
                               da.zeros((total_entries, ensemble_size, lead_time, nlat, nlon), chunks=(1, 1, lead_time, nlat, nlon), dtype=dataset_dtype))
        for ac in atmospheric_channel_names_wb2:
            data_arrays[ac] = (["time", "number", "prediction_timedelta", "level", "latitude", "longitude"],
			       da.zeros((total_entries, ensemble_size, lead_time, nlevels, nlat, nlon), chunks=(1, 1, lead_time, nlevels, nlat, nlon), dtype=dataset_dtype))

        # create dataset
        datastore = xr.Dataset(data_arrays,
                               coords={
                                   "time": timestamps,
                                   "number": np.arange(1, ensemble_size+1, dtype=np.int32),
                                   "prediction_timedelta": lead_times,
                                   "level": np.array(atmospheric_levels, dtype=np.int32),
                                   "latitude": latitudes,
                                   "longitude": longitudes,
                               })
        datastore.to_zarr(store=output_file, mode='w', compute=False)

        pbar = progressbar.ProgressBar(maxval=total_entries)
        pbar.update(0)

    # we need to wait here
    comm.Barrier()
        
    global_off = 0
    for filename, ne in zip(file_names_to_convert, entries_per_year):

        # compute chunking across MPI ranks:
        ne_local = (ne + comm_size - 1) // comm_size
        ne_start = ne_local * comm_rank
        ne_end = min(ne_start + ne_local, ne)
        # update local size
        ne_local = ne_end - ne_start
        # protect against negative batch sizes which happen on ranks which have nothing to do
        batch_size = max(min(batch_size, ne_local), 1)
        
        with h5.File(filename, "r") as f:
            
            # loop over batches
            for ioff in range(ne_start, ne_end, batch_size):
            
                # shape
                start = global_off + ioff
                end = min(start + batch_size, ne_end)
                nsamples = end - start

                if verbose:
                    print(f"{comm_rank}: file={filename}, ne={ne}, start={start}, end={end}, ne_start={ne_start}, ne_end={ne_end}, ne_local={ne_local}, batch_size={batch_size}")
                
                # surface channel
                chunk_data_arrays = {}
                for sc in surface_channel_names:
                    idc = channel_names.index(sc)
                    data = f[entry_key][start:end, :, :, idc, ...]
                    # transpose dims 1 und 2
                    data = np.transpose(data, axes=(0, 2, 1, 3, 4))
                    
                    # create xarray
                    scwb2 = surface_variables[sc]
                    chunk_data_arrays[scwb2] = (["time", "number", "prediction_timedelta", "latitude", "longitude"], data)

                # atmospheric levels
                for ac in atmospheric_channel_names:
                    data = np.empty((nsamples, ensemble_size, lead_time, nlevels, nlat, nlon), dtype=dataset_dtype)
                    for idl, level in enumerate(atmospheric_levels):
                        idc = channel_names.index(ac+str(level))
                        tmpdata = f[entry_key][start:end, :, :, idc, :, :]
                        tmpdata = np.transpose(tmpdata, axes=(0, 2, 1, 3, 4))
                        data[..., idl, :, :] = tmpdata[...]
                    
                    # create array
                    acwb2 = atmospheric_variables[ac]
                    chunk_data_arrays[acwb2] = (["time", "number", "prediction_timedelta", "level", "latitude", "longitude"], data)
                
                # write the data
                chunk_data = xr.Dataset(chunk_data_arrays)
                chunk_data.to_zarr(
                    store=output_file,
                    region={
                        "time": slice(start, end),
                        "number": slice(0, ensemble_size),
                        "prediction_timedelta": slice(0, lead_time),
                        "level": slice(0, nlevels),
                        "latitude": slice(0, nlat),
                        "longitude": slice(0, nlon),
                    },
                    mode="a"
                )

            # we need a barrier here
            comm.Barrier()
                
            # modify offset
            global_off += ne

            # update progressbar
            if comm_rank == 0:
                pbar.update(global_off)

    # end time
    end_time = time.perf_counter()
    run_time = str(dt.timedelta(seconds=end_time-start_time))

    if comm_rank == 0:
        pbar.finish()
        print(f"All done. Run time {run_time}.")

    comm.Barrier()

    return


def main(args):
    # get files
    files = glob(os.path.join(args.input_dir, "*.h5"))

    if not files:
        raise RuntimeError("The directory input_dir has to contain h5 files")
    
    # concatenate files with timestamp information
    convert(file_names_to_convert=files,
            output_file=args.output_file,
            batch_size=args.batch_size,
            verbose=args.verbose)


if __name__ == '__main__':

    # argparse
    parser = ap.ArgumentParser()
    parser.add_argument("--input_dir", type=str, help="Directory with HDF5 input files.", required=True)
    parser.add_argument("--output_file", type=str, help="Filename for saving wb2 compatible zarr file file.", required=True)
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for writing chunks")
    parser.add_argument("--verbose", action="store_true")
    args = parser.parse_args()
    
    main(args)
