# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import os
import unittest
from parameterized import parameterized

import torch
from makani.models.common import RealFFT1, InverseRealFFT1, RealFFT2, InverseRealFFT2, RealFFT3, InverseRealFFT3
from makani.mpu.fft import DistributedRealFFT1, DistributedInverseRealFFT1, DistributedRealFFT2, DistributedInverseRealFFT2, DistributedRealFFT3, DistributedInverseRealFFT3

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from .distributed_helpers import init_grid, split_helper, gather_helper
from ..testutils import compare_tensors

class TestDistributedRealFFT(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        init_grid(cls)


    def _split_helper(self, tensor):
        tensor_local = split_helper(tensor, dim=-1, group=self.w_group)
        tensor_local = split_helper(tensor_local, dim=-2, group=self.h_group)

        return tensor_local
        
        
    def _gather_helper(self, tensor):
        tensor_gather = gather_helper(tensor, dim=-2, group=self.h_group)
        tensor_gather =	gather_helper(tensor_gather, dim=-1, group=self.w_group)
        
        return tensor_gather

    @parameterized.expand(
        [
            [256, 512, 32,  8, 1e-6],
            [361, 720,  1, 10, 1e-6],
        ],
        skip_on_empty=True,
    )
    def test_distributed_fft1(self, nlat, nlon, batch_size, num_chan, tol, verbose=False):
        B, C, H, W = batch_size, num_chan, nlat, nlon

        # set up handles
        forward_transform_local = RealFFT1(nlon=W).to(self.device)
        forward_transform_dist = DistributedRealFFT1(nlon=W).to(self.device)

        # create tensors
        inp_full = torch.randn((B, C, H, W), dtype=torch.float32, device=self.device)

        #############################################################
        # local transform
        #############################################################
        # FWD pass
        inp_full.requires_grad = True
        out_full = forward_transform_local(inp_full)

        # create grad for backward
        with torch.no_grad():
            # create full grad
            ograd_full = torch.randn_like(out_full)
            
        # BWD pass
        out_full.backward(ograd_full)
        igrad_full = inp_full.grad.clone()
        
        #############################################################
        # distributed transform
        #############################################################
        # FWD pass
        inp_local = self._split_helper(inp_full)
        inp_local.requires_grad = True
        out_local = forward_transform_dist(inp_local)

        # BWD pass
        ograd_local = self._split_helper(ograd_full)
        out_local = forward_transform_dist(inp_local)
        out_local.backward(ograd_local)
        igrad_local = inp_local.grad.clone()
        
        #############################################################
        # evaluate FWD pass
        #############################################################
        with self.subTest(desc="output"):
            out_gather_full = self._gather_helper(out_local)
            self.assertTrue(compare_tensors("output", out_gather_full, out_full, tol, tol, verbose=verbose))

        #############################################################
        # evaluate BWD pass
        #############################################################
        with self.subTest(desc="input gradients"):
            igrad_gather_full = self._gather_helper(igrad_local)
            self.assertTrue(compare_tensors("input gradients", igrad_gather_full, igrad_full, tol, tol, verbose=verbose))

    @parameterized.expand(
        [
            [256, 512, 32,  8, 1e-6],
            [361, 720,  1, 10, 1e-6],
        ],
        skip_on_empty=True,
    )
    def test_distributed_ifft1(self, nlat, nlon, batch_size, num_chan, tol, verbose=False):
        B, C, H, W = batch_size, num_chan, nlat, nlon

        forward_transform_local = RealFFT1(nlon=W).to(self.device)
        backward_transform_local = InverseRealFFT1(nlon=W).to(self.device)
        backward_transform_dist = DistributedInverseRealFFT1(nlon=W).to(self.device)

        # create tensors
        dummy_full = torch.randn((B, C, H, W), dtype=torch.float32, device=self.device)
        inp_full = forward_transform_local(dummy_full)

        #############################################################
        # local transform
	    #############################################################
	    # FWD pass
        inp_full.requires_grad = True
        out_full = backward_transform_local(inp_full)

        # create grad for backward
        with torch.no_grad():
            # create full grad
            ograd_full = torch.randn_like(out_full)

        # BWD pass
        out_full.backward(ograd_full)

        # repeat once due to known irfft bug
        inp_full.grad = None
        out_full = backward_transform_local(inp_full)
        out_full.backward(ograd_full)
        igrad_full = inp_full.grad.clone()

        #############################################################
        # distributed transform
        #############################################################
        # FWD pass
        inp_local = self._split_helper(inp_full)
        inp_local.requires_grad = True
        out_local = backward_transform_dist(inp_local)

        # BWD pass
        ograd_local = self._split_helper(ograd_full)
        out_local = backward_transform_dist(inp_local)
        out_local.backward(ograd_local)
        igrad_local = inp_local.grad.clone()
        
        #############################################################
        # evaluate FWD pass
        #############################################################
        with self.subTest(desc="output"):
            out_gather_full = self._gather_helper(out_local)
            self.assertTrue(compare_tensors("output", out_gather_full, out_full, tol, tol, verbose=verbose))

        #############################################################
        # evaluate BWD pass
        #############################################################
        with self.subTest(desc="input gradients"):
            igrad_gather_full = self._gather_helper(igrad_local)
            self.assertTrue(compare_tensors("input gradients", igrad_gather_full, igrad_full, tol, tol, verbose=verbose))


    @parameterized.expand(
        [
            [256, 512, 0, 32,  8, 1e-6],
            [361, 720, 0,  1, 10, 1e-6],
            [256, 512, 4, 32,  8, 1e-6],
            [361, 720, 4,  1, 10, 1e-6],
        ],
        skip_on_empty=True,
    )
    def test_distributed_fft2_3(self, nlat, nlon, nalt, batch_size, num_chan, tol, verbose=False):
        B, C, D, H, W = batch_size, num_chan, nalt, nlat, nlon

        # set up handles
        if D > 0:
            forward_transform_local = RealFFT3(nd=D, nh=H, nw=W).to(self.device)
            forward_transform_dist = DistributedRealFFT3(nd=D, nh=H, nw=W).to(self.device)
        else:
            forward_transform_local = RealFFT2(nlat=H, nlon=W).to(self.device)
            forward_transform_dist = DistributedRealFFT2(nlat=H, nlon=W).to(self.device)

        # create tensors
        if D > 0:
            inp_full = torch.randn((B, C, D, H, W), dtype=torch.float32, device=self.device)
        else:
            inp_full = torch.randn((B, C, H, W), dtype=torch.float32, device=self.device)

        #############################################################
        # local transform
        #############################################################
        # FWD pass
        inp_full.requires_grad = True
        out_full = forward_transform_local(inp_full)

        # create grad for backward
        with torch.no_grad():
            # create full grad
            ograd_full = torch.randn_like(out_full)
            
        # BWD pass
        out_full.backward(ograd_full)
        igrad_full = inp_full.grad.clone()
        
        #############################################################
        # distributed transform
        #############################################################
        # FWD pass
        inp_local = self._split_helper(inp_full)
        inp_local.requires_grad = True
        out_local = forward_transform_dist(inp_local)

        # BWD pass
        ograd_local = self._split_helper(ograd_full)
        out_local = forward_transform_dist(inp_local)
        out_local.backward(ograd_local)
        igrad_local = inp_local.grad.clone()
        
        #############################################################
        # evaluate FWD pass
        #############################################################
        with self.subTest(desc="output"):
            out_gather_full = self._gather_helper(out_local)
            self.assertTrue(compare_tensors("output", out_gather_full, out_full, tol, tol, verbose=verbose))

        #############################################################
        # evaluate BWD pass
        #############################################################
        with self.subTest(desc="input gradients"):
            igrad_gather_full = self._gather_helper(igrad_local)
            self.assertTrue(compare_tensors("input gradients", igrad_gather_full, igrad_full, tol, tol, verbose=verbose))


    @parameterized.expand(
        [
            [256, 512, 0, 32,  8, 5e-6],
            [361, 720, 0,  1, 10, 5e-6],
            [256, 512, 4, 32,  8, 5e-6],
            [361, 720, 4,  1, 10, 5e-6],
        ],
        skip_on_empty=True,
    )
    def test_distributed_ifft2_3(self, nlat, nlon, nalt, batch_size, num_chan, tol, verbose=True):
        B, C, D, H, W = batch_size, num_chan, nalt, nlat, nlon

        if D > 0:
            forward_transform_local = RealFFT3(nd=D, nh=H, nw=W).to(self.device)
            backward_transform_local = InverseRealFFT3(nd=D, nh=H, nw=W).to(self.device)
            backward_transform_dist = DistributedInverseRealFFT3(nd=D, nh=H, nw=W).to(self.device)
        else:
            forward_transform_local = RealFFT2(nlat=H, nlon=W).to(self.device)
            backward_transform_local = InverseRealFFT2(nlat=H, nlon=W).to(self.device)
            backward_transform_dist = DistributedInverseRealFFT2(nlat=H, nlon=W).to(self.device)

        # create tensors
        if D > 0:
            dummy_full = torch.randn((B, C, D, H, W), dtype=torch.float32, device=self.device)
        else:
            dummy_full = torch.randn((B, C, H, W), dtype=torch.float32, device=self.device)
        inp_full = forward_transform_local(dummy_full)

        #############################################################
        # local transform
        #############################################################
        # FWD pass
        inp_full.requires_grad = True
        out_full = backward_transform_local(inp_full)

        # create grad for backward
        with torch.no_grad():
            # create full grad
            ograd_full = torch.randn_like(out_full)

        # BWD pass
        out_full.backward(ograd_full)

        # repeat once due to known irfft bug
        inp_full.grad = None
        out_full = backward_transform_local(inp_full)
        out_full.backward(ograd_full)
        igrad_full = inp_full.grad.clone()

        #############################################################
        # distributed transform
        #############################################################
        # FWD pass
        inp_local = self._split_helper(inp_full)
        inp_local.requires_grad = True
        out_local = backward_transform_dist(inp_local)

        # BWD pass
        ograd_local = self._split_helper(ograd_full)
        out_local = backward_transform_dist(inp_local)
        out_local.backward(ograd_local)
        igrad_local = inp_local.grad.clone()
        
        #############################################################
        # evaluate FWD pass
        #############################################################
        with self.subTest(desc="output"):
            out_gather_full = self._gather_helper(out_local)
            self.assertTrue(compare_tensors("output", out_gather_full, out_full, tol, tol, verbose=verbose))

        #############################################################
        # evaluate BWD pass
        #############################################################
        with self.subTest(desc="input gradients"):
            igrad_gather_full = self._gather_helper(igrad_local)
            self.assertTrue(compare_tensors("input gradients", igrad_gather_full, igrad_full, tol, tol, verbose=verbose))

if __name__ == '__main__':    
    unittest.main()
